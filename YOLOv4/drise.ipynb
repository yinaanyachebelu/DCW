{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56982503-47fb-4dd6-8a27-10fbc8d32fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Migrating database to v0.21.6\n",
      "Downloading dataset to '/Users/ayinaanyachebelu/fiftyone/quickstart'\n",
      "Downloading dataset...\n",
      " 100% |████|  187.5Mb/187.5Mb [1.1s elapsed, 0s remaining, 183.8Mb/s]         \n",
      "Extracting dataset...\n",
      "Parsing dataset metadata\n",
      "Found 200 samples\n",
      "Dataset info written to '/Users/ayinaanyachebelu/fiftyone/quickstart/info.json'\n",
      "Loading 'quickstart'\n",
      " 100% |█████████████████| 200/200 [2.6s elapsed, 0s remaining, 70.9 samples/s]       \n",
      "Dataset 'quickstart' created\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.google_utils import attempt_load\n",
    "from utils.datasets import create_dataloader\n",
    "from utils.general import coco80_to_coco91_class, check_dataset, check_file, check_img_size, box_iou, \\\n",
    "    non_max_suppression, scale_coords, xyxy2xywh, xywh2xyxy, clip_coords, set_logging, increment_path\n",
    "from utils.loss import compute_loss\n",
    "from utils.metrics import ap_per_class\n",
    "from utils.plots import plot_images, output_to_target\n",
    "from utils.torch_utils import select_device, time_synchronized\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "%matplotlib inline\n",
    "\n",
    "from models.models import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f812b24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(prog='test.py')\n",
    "parser.add_argument('--weights', nargs='+', type=str, \n",
    "                    default='/home/ayina/MscThesis/DCW/YOLOv4/runs/train/yolov4_08/weights/best.pt')\n",
    "parser.add_argument('--data', type=str, default='./data/test_coco.yaml', help='*.data path')\n",
    "parser.add_argument('--batch-size', type=int, default=1, help='size of each image batch')\n",
    "parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')\n",
    "parser.add_argument('--conf-thres', type=float, default=0.001, help='object confidence threshold')\n",
    "parser.add_argument('--iou-thres', type=float, default=0.65, help='IOU threshold for NMS')\n",
    "parser.add_argument('--task', default='600', help=\"'val', 'test', 'study'\")\n",
    "parser.add_argument('--device', default='0', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
    "parser.add_argument('--single-cls', action='store_true', help='treat as single-class dataset')\n",
    "parser.add_argument('--augment', action='store_true', help='augmented inference')\n",
    "parser.add_argument('--verbose', action='store_true', help='report mAP by class')\n",
    "parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n",
    "parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')\n",
    "parser.add_argument('--save-json', action='store_true', help='save a cocoapi-compatible JSON results file')\n",
    "parser.add_argument('--project', default='runs/test', help='save to project/name')\n",
    "parser.add_argument('--name', default='exp', help='save to project/name')\n",
    "parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n",
    "parser.add_argument('--cfg', type=str, default='./cfg/yolov4.cfg', help='*.cfg path')\n",
    "parser.add_argument('--names', type=str, default='./data/weed.names', help='*.cfg path')\n",
    "opt = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3690aa-a687-4505-b909-8a95d91e1e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_classes(path):\n",
    "    # Loads *.names file at 'path'\n",
    "    with open(path, 'r') as f:\n",
    "        names = f.read().split('\\n')\n",
    "    return list(filter(None, names))  # filter removes empty strings (such as last line)\n",
    "\n",
    "cats = [\n",
    "    \"N/A\",\n",
    "    \"Waterhemp\",\n",
    "    \"MorningGlory\",\n",
    "    \"Purslane\",\n",
    "    \"SpottedSpurge\",\n",
    "    \"Carpetweed\",\n",
    "    \"Ragweed\",\n",
    "    \"Eclipta\",\n",
    "    \"PricklySida\",\n",
    "    \"PalmerAmaranth\",\n",
    "    \"Sicklepod\",\n",
    "    \"Goosegrass\",\n",
    "    \"CutleafGroundcherry\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79c43ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = [\n",
    "    \"N/A\",\n",
    "    \"Waterhemp\",\n",
    "    \"MorningGlory\",\n",
    "    \"Purslane\",\n",
    "    \"SpottedSpurge\",\n",
    "    \"Carpetweed\",\n",
    "    \"Ragweed\",\n",
    "    \"Eclipta\",\n",
    "    \"PricklySida\",\n",
    "    \"PalmerAmaranth\",\n",
    "    \"Sicklepod\",\n",
    "    \"Goosegrass\",\n",
    "    \"CutleafGroundcherry\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c22601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(image_size, grid_size, prob_thresh):\n",
    "    image_w, image_h = image_size\n",
    "    grid_w, grid_h = grid_size\n",
    "    cell_w, cell_h = math.ceil(image_w / grid_w), math.ceil(image_h / grid_h)\n",
    "    up_w, up_h = (grid_w + 1) * cell_w, (grid_h + 1) * cell_h\n",
    "\n",
    "    mask = (np.random.uniform(0, 1, size=(grid_h, grid_w)) <\n",
    "            prob_thresh).astype(np.float32)\n",
    "    mask = cv2.resize(mask, (up_w, up_h), interpolation=cv2.INTER_LINEAR)\n",
    "    offset_w = np.random.randint(0, cell_w)\n",
    "    offset_h = np.random.randint(0, cell_h)\n",
    "    mask = mask[offset_h:offset_h + image_h, offset_w:offset_w + image_w]\n",
    "    return mask\n",
    "\n",
    "def mask_image(image, mask):\n",
    "    masked = ((image.astype(np.float32) / 255 * np.dstack([mask] * 3)) * 255).astype(np.uint8)\n",
    "    return masked\n",
    "\n",
    "def iou(box1, box2):\n",
    "    box1 = np.asarray(box1)\n",
    "    box2 = np.asarray(box2)\n",
    "    tl = np.vstack([box1[:2], box2[:2]]).max(axis=0)\n",
    "    br = np.vstack([box1[2:], box2[2:]]).min(axis=0)\n",
    "    intersection = np.prod(br - tl) * np.all(tl < br).astype(float)\n",
    "    area1 = np.prod(box1[2:] - box1[:2])\n",
    "    area2 = np.prod(box2[2:] - box2[:2])\n",
    "    return intersection / (area1 + area2 - intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17673b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(data,\n",
    "         weights=None,\n",
    "         batch_size=8,\n",
    "         imgsz=640,\n",
    "         conf_thres=0.001,\n",
    "         iou_thres=0.6,  # for NMS\n",
    "         save_json=True,\n",
    "         single_cls=False,\n",
    "         augment=False,\n",
    "         verbose=False,\n",
    "         model=None,\n",
    "         dataloader=None,\n",
    "         save_dir=Path(''),  # for saving images\n",
    "         save_txt=False,  # for auto-labelling\n",
    "         save_conf=False,\n",
    "         plots=True,\n",
    "         log_imgs=0,\n",
    "        max_size=640):  # number of logged images\n",
    "\n",
    "    # Initialize/load model and set device\n",
    "    training = model is not None\n",
    "    if training:  # called by train.py\n",
    "        device = next(model.parameters()).device  # get model device\n",
    "\n",
    "    else:  # called directly\n",
    "        #set_logging()\n",
    "        device = select_device(opt.device, batch_size=batch_size)\n",
    "\n",
    "        # Load model\n",
    "        model = Darknet(opt.cfg).to(device)\n",
    "\n",
    "        try:\n",
    "            ckpt = torch.load(weights, map_location=device)  # load checkpoint\n",
    "            ckpt['model'] = {k: v for k, v in ckpt['model'].items() if model.state_dict()[k].numel() == v.numel()}\n",
    "            model.load_state_dict(ckpt['model'], strict=False)\n",
    "        except:\n",
    "            load_darknet_weights(model, weights[0])\n",
    "        imgsz = check_img_size(imgsz, s=64)  # check img_size\n",
    "\n",
    "    # Half\n",
    "    half = device.type != 'cpu'  # half precision only supported on CUDA\n",
    "    if half:\n",
    "        model.half()\n",
    "\n",
    "    # Configure\n",
    "    model.eval()\n",
    "    is_coco = data.endswith('coco.yaml')  # is COCO dataset\n",
    "    with open(data) as f:\n",
    "        data = yaml.load(f, Loader=yaml.FullLoader)  # model dict\n",
    "    check_dataset(data)  # check\n",
    "    nc = 1 if single_cls else int(data['nc'])  # number of classes\n",
    "    iouv = torch.linspace(0.5, 0.95, 10).to(device)  # iou vector for mAP@0.5:0.95\n",
    "    niou = iouv.numel()\n",
    "\n",
    "    # Dataloader\n",
    "    if not training:\n",
    "        img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n",
    "        _ = model(img.half() if half else img) if device.type != 'cpu' else None  # run once\n",
    "        path = data['test'] \n",
    "        dataloader = create_dataloader(path, imgsz, batch_size, 64, opt, pad=0.5, rect=True)[0]\n",
    "\n",
    "    seen = 0\n",
    "    try:\n",
    "        names = model.names if hasattr(model, 'names') else model.module.names\n",
    "    except:\n",
    "        names = load_classes(opt.names)\n",
    "    coco91class = coco80_to_coco91_class()\n",
    "    \n",
    "    ######################################################################\n",
    "    \n",
    "    for batch_i, (img, targets, paths, shapes) in enumerate(dataloader):\n",
    "        path = Path(paths[0])\n",
    "\n",
    "        file_name = f\"{int(opt.task):012d}.jpg\"\n",
    "        src_path = \"/home/ayina/MscThesis/DCW/datasets/Dataset_final/DATA_0_YOLO/images/\"\n",
    "        img_path = src_path + file_name\n",
    "        \n",
    "        if str(path) != img_path:\n",
    "            continue\n",
    "        \n",
    "        print(\"path: \", img_path)\n",
    "        nb, _, height, width = img.shape  # batch size, channels, height, width\n",
    "        res = np.zeros((height, width), dtype=np.float32)\n",
    "        whwh = torch.Tensor([width, height, width, height])\n",
    "        \n",
    "        image_targets = targets\n",
    "        labels = image_targets[image_targets[:, 0] == 0, 1:]\n",
    "        target_classes = image_targets[:, 1].numpy().astype('int')\n",
    "        tbox = xywh2xyxy(labels[:, 1:5]) * whwh\n",
    "        tbox = tbox.numpy()\n",
    "        \n",
    "        # set parameters for saliency map creation\n",
    "        \n",
    "        target_class_index = 1\n",
    "        misclass = 9\n",
    "        \n",
    "        for j, box in enumerate(tbox):\n",
    "            cat_id = int(target_classes[j])+1\n",
    "            if cat_id == target_class_index:\n",
    "                target_box = box\n",
    "        \n",
    "        target_box = target_box.astype(int)\n",
    "        prob_thresh= 0.25\n",
    "        grid_size= (16, 16)\n",
    "        n_masks= 1750\n",
    "        \n",
    "        print(\"target_box \", target_box)\n",
    "        \n",
    "        # original image for plotting later\n",
    "        \n",
    "        if isinstance(img, torch.Tensor):\n",
    "            image = img.cpu().float().numpy()\n",
    "                \n",
    "        if np.max(image[0]) <= 1:\n",
    "            image *= 255\n",
    "            \n",
    "        image = image[0]\n",
    "        image = image.transpose(1, 2, 0)\n",
    "        \n",
    "        \n",
    "        # generate masked images\n",
    "        \n",
    "        if isinstance(img, torch.Tensor):\n",
    "            image_np = img[0].float().numpy()\n",
    "            image_np = image_np.transpose(1, 2, 0)\n",
    "            \n",
    "        for _ in range(n_masks):\n",
    "            mask = generate_mask(image_size=(width, height),\n",
    "                                 grid_size=grid_size,\n",
    "                                 prob_thresh=prob_thresh)\n",
    "            masked = mask_image(image_np, mask)\n",
    "\n",
    "            # prepare masked image for inference\n",
    "\n",
    "            masked = masked.transpose((2, 0, 1))\n",
    "            masked = torch.from_numpy(masked)\n",
    "            masked = masked.unsqueeze(0)\n",
    "            masked = masked.to(device, non_blocking=True)\n",
    "            masked = masked.half() if half else masked.float()\n",
    "            masked /= 255.0\n",
    "            \n",
    "            # Run model\n",
    "            with torch.no_grad():\n",
    "                inf_out, train_out = model(masked, augment=augment)  # inference and training outputs\n",
    "                output = non_max_suppression(inf_out, conf_thres=conf_thres, iou_thres=iou_thres)\n",
    "                \n",
    "            max_iou_score = 0\n",
    "            \n",
    "            for si, pred in enumerate(output):\n",
    "                labels = targets[targets[:, 0] == si, 1:]\n",
    "                nl = len(labels)\n",
    "                tcls = labels[:, 0].tolist() if nl else []  # target class\n",
    "                seen += 1\n",
    "\n",
    "                box = pred[:, :4].clone()  # xyxy\n",
    "\n",
    "                for p, b in zip(pred.tolist(), box.tolist()):\n",
    "                    cat_id = coco91class[int(p[5])] if is_coco else int(p[5])\n",
    "                    cat = cats[cat_id]\n",
    "                    bbox = [int(x) for x in b]\n",
    "                    bboxa = np.asarray(bbox)\n",
    "                    score = round(p[4], 5)\n",
    "                    \n",
    "                    if cat_id == misclass:\n",
    "                        #print(\"bbox: \", bbox)\n",
    "                        iou_score = iou(target_box, bbox) * score\n",
    "                        if iou_score > max_iou_score:\n",
    "                            max_iou_score = iou_score \n",
    "                        \n",
    "                res += mask * max_iou_score\n",
    "\n",
    "        fig = plt.figure(figsize=(6, 4))\n",
    "        fig.add_subplot()\n",
    "        ax = plt.gca()\n",
    "        ax.imshow(image.astype('uint8'))\n",
    "        ax.add_patch(plt.Rectangle((target_box[0], target_box[1]), target_box[2] - target_box[0], \n",
    "                                   target_box[3] - target_box[1],\n",
    "                                        fill=False, color='white', linewidth=3))\n",
    "        ax.imshow(res, cmap='jet', alpha=0.4)\n",
    "            \n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913093d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "mask(opt.data,\n",
    "         opt.weights,\n",
    "         opt.batch_size,\n",
    "         opt.img_size,\n",
    "         opt.conf_thres,\n",
    "         opt.iou_thres,\n",
    "         opt.save_json,\n",
    "         opt.single_cls,\n",
    "         opt.augment,\n",
    "         opt.verbose,\n",
    "         save_txt=opt.save_txt,\n",
    "         save_conf=opt.save_conf,\n",
    "         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('geospatial')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "fb590520fd928bf6bc422d021ab4a40dca68e545a4967c067fe947681b572fb6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
